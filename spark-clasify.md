1. How many rows and columns does the data have?
   * The data has 2946 rows and 47 columns. 

2. How many different values does the last column have?
   * The last column has 5 different values (0 to 4 to be specific) 

3. How many rows are there with each of those values?

   |c46 |count|
   |----|-----|
   |   0|  642|
   |   1|   15|
   |   2| 1068|
   |   3|   29|
   |   4| 1192|

4. Using the corr method of the org.apache.spark.ml.stat.Correlation object, calculate the correlation matrix for the numeric columns. The example at https://spark.apache.org/docs/latest/ml-statistics.html could be very helpful for figuring out how to do this. Copy the matrix into the email.
   * Pearson correlation matrix:
 1.0                    -0.08515838753061737    ... (47 total)
-0.08515838753061737   1.0                     ...
0.04864301699458359    0.020745978631789064    ...
0.07788055691717045    0.030339382185620118    ...
-0.038256043338390874  -0.021293518795742903   ...
NaN                    NaN                     ...
NaN                    NaN                     ...
NaN                    NaN                     ...
-0.20070844005479277   0.035765348954218325    ...
-0.005310303580434085  -0.009281813515603931   ...
0.2138458271167989     -0.03390513446934602    ...
-0.0663319583048558    0.02248517179414137     ...
-0.024986434524609948  -0.1351385102116024     ...
-0.013489578735152545  -0.11517999067437193    ...
-0.002864834693822084  -0.21448733797201666    ...
-0.00936550080019635   -0.07781493561198714    ...
-0.021153115353745387  -0.07612411805201164    ...
-0.022667761367743712  -0.047806953612249674   ...
-0.01878030281384982   -0.03431407562141609    ...
-0.005981374138126529  0.0539670420002239      ...
0.005732266521715227   0.061688661186114845    ...
0.11162136155543918    -0.06491995874623059    ...
-0.17361678674122746   0.06636868327360329     ...
-0.02147519297825607   -0.026825160929296276   ...
-0.16161681131110445   0.09584379076958775     ...
-0.12248223644003566   -0.020870424595721363   ...
-0.14600858261147293   0.04472490513228729     ...
-0.031070294756292798  0.21091848999131516     ...
-0.13063687754720912   0.016986389798388384    ...
-0.13387557109070738   0.036273444422168934    ...
-0.14262707658823717   0.031701218742845906    ...
-0.11437103957073758   -0.0022440113261264975  ...
-0.12544681296841134   4.926581663905588E-4    ...
-0.04261155285196958   0.02831190248283465     ...
-0.047877062587653355  0.03267727893124143     ...
-0.03715784953044165   0.02382301444515146     ...
-0.04001470108666611   0.03767937467018678     ...
-0.087114747441419     -0.018000158454320427   ...
-0.09729988052092035   -0.008857874726686215   ...
-0.07645819753674062   -0.02709325945922537    ...
0.37926750446561813    -0.07039993999964385    ...
0.5191423641350752     -0.027998319224703288   ...
-0.22999990404550813   -0.1181082983722855     ...
-0.07410180636624072   0.06608035982777133     ...
0.022995059460674778   0.09325971658277293     ...
-0.08801904820580857   -0.01911295581383116    ...
0.06250509305309454    -0.0576096610556067     ...


5. Based on the correlation matrix, which three columns are most highly correlated (or anti-correlated) with the last column?
  * Column 22 has a correlation of -0.3885121083933726. Column 23 has a correlation of -0.21478089709003176. Column 26 has a correlation of -0.09844073158570038. Those are the three columns that has the highest correlation with the last column. 


6. Find the best classification scheme you can for the last column. There are two ways to interpret the classification, and I want you to do those independently. (See a and b below.) Also, include an explanation of what the key elements are for the classification. You should probably try a few of the classifiers to see which ones work best, but your explanation of factors might use a classifier that isn't optimal.
   * a. Classify all values for the last column.
   * b. Classify the last column into two sets: 0 and 1 vs 2, 3, and 4.

 |   Classifier          | Binary Accurarcy| Multiclass accuracy|
 |-----------------------|-----------------|--------------------|
 | Random Forest         |     87.18%      |      50.51%        |
 | Gradient Boosted Tree |     87.57%      |      NaN           | 
 | Multilayer Perceptron |     NaN         |      6.3%          | 
 | Linear SVC            |     85.62%      |      NaN           | 
 | One VS Rest           |     NaN         |      52.99%        | 
 | Naive Bayes           |     50.40%      |      23.84%        | 

 * a) The best for multiclass classfication is one vs rest, with nearly 53% accuracy. Random forest is also a good choice. Random forest can create a multitude of decision trees at training time and output the class thta is the mode of classification or regression of the individual trees. It can achieve pretty good accuracies if trained properly. One vs rest involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a class label;

 
 * b) The best for binary classification is gradient boosted tree and random forest, each with over 87% accuracy. Gradient boosted tree is similar to random forest algorithm. 
